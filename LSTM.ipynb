{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Ucj2To_moN"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.nn import functional as f\r\n",
        "\r\n",
        "class LSTMClassifier(nn.Module):\r\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\r\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\r\n",
        "\t\r\n",
        "\t\tself.batch_size = batch_size\r\n",
        "\t\tself.output_size = output_size\r\n",
        "\t\tself.hidden_size = hidden_size\r\n",
        "\t\tself.vocab_size = vocab_size\r\n",
        "\t\tself.embedding_length = embedding_length\r\n",
        "\t\t\r\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\r\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\r\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\r\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\r\n",
        "\t\t\r\n",
        "\tdef forward(self, input_sentence, batch_size=None):\r\n",
        "   \r\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\r\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\r\n",
        "\t\tif batch_size is None:\r\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\r\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\r\n",
        "\t\telse:\r\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\r\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\r\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\r\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\r\n",
        "\t\t\r\n",
        "\t\treturn final_output"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}