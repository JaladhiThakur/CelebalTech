{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktDNxl0IkqDT"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "class RNN(nn.Module):\r\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\r\n",
        "\t\tsuper(RNN, self).__init__()\r\n",
        "\r\n",
        "\t\tself.batch_size = batch_size\r\n",
        "\t\tself.output_size = output_size\r\n",
        "\t\tself.hidden_size = hidden_size\r\n",
        "\t\tself.vocab_size = vocab_size\r\n",
        "\t\tself.embedding_length = embedding_length\r\n",
        "\t\t\r\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\r\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\r\n",
        "\t\tself.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\r\n",
        "\t\tself.label = nn.Linear(4*hidden_size, output_size)\r\n",
        "\t\r\n",
        "\tdef forward(self, input_sentences, batch_size=None):\r\n",
        "\r\n",
        "\t\tinput = self.word_embeddings(input_sentences)\r\n",
        "\t\tinput = input.permute(1, 0, 2)\r\n",
        "\t\tif batch_size is None:\r\n",
        "\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda()) # 4 = num_layers*num_directions\r\n",
        "\t\telse:\r\n",
        "\t\t\th_0 =  Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\r\n",
        "\t\toutput, h_n = self.rnn(input, h_0)\r\n",
        "\t\t# h_n.size() = (4, batch_size, hidden_size)\r\n",
        "\t\th_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\r\n",
        "\t\th_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\r\n",
        "\t\t# h_n.size() = (batch_size, 4*hidden_size)\r\n",
        "\t\tlogits = self.label(h_n) # logits.size() = (batch_size, output_size)\r\n",
        "\t\t\r\n",
        "\t\treturn logits"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}